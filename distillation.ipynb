{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7895ebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialize the path directories for the student and teacher model and also the name\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "TEACHER_PATH = \"./drive/MyDrive/bert_sst2_baseline/best_model\"\n",
    "STUDENT_NAME = \"bert_sst2_student\"\n",
    "OUTPUT_DIR = \"./drive/MyDrive/bert_sst2_student/best_model\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# training hyperparameters\n",
    "num_epochs = 4\n",
    "per_device_train_batch_size = 16\n",
    "per_device_eval_batch_size = 64\n",
    "learning_rate = 3e-5\n",
    "weight_decay = 0.01\n",
    "warmup_ratio = 0.06\n",
    "\n",
    "temperature = 2.0\n",
    "alpha = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b043c0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "tasks = \"sst2\"\n",
    "dataset = load_dataset(\"glue\", tasks)\n",
    "metric = load(\"accuracy\")\n",
    "\n",
    "# same as before, load the tokenizer and tokenize the dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"sentence\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence\", \"idx\"])\n",
    "tokenized_datasets.set_format(type=\"torch\")\n",
    "# split into three\n",
    "train_datasets = tokenized_datasets[\"train\"]\n",
    "val_datasets = tokenized_datasets[\"validation\"]\n",
    "test_datasets = tokenized_datasets[\"test\"]\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa09bb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher = AutoModelForSequenceClassification.from_pretrained(TEACHER_PATH, num_labels=2, local_files_only=True)\n",
    "teacher.to(DEVICE)\n",
    "teacher.eval()\n",
    "for p in teacher.parameters():\n",
    "    p.requires_grad = False\n",
    "# pretrained-student\n",
    "PRETRAINED_STUDENT = \"distilbert-base-uncased\"\n",
    "student = AutoModelForSequenceClassification.from_pretrained(PRETRAINED_STUDENT, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83100183",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationTrainer(Trainer):\n",
    "    def __init__(self, teacher_model=None, temperature=1.0, alpha=0.5, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher = teacher_model\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "        self.kl_loss = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Compute combined CE + KL distillation loss\n",
    "        Accept **kwargs to avoid unexpected keyword errors from Trainer.\n",
    "        \"\"\"\n",
    "        labels = inputs.get(\"labels\")\n",
    "        inputs = {k: v for k, v in inputs.items()}\n",
    "        inputs.pop(\"token_type_ids\", None)\n",
    "\n",
    "        # forward for student (exclude labels when passing to model)\n",
    "        student_inputs = {k: v for k, v in inputs.items() if k != \"labels\"}\n",
    "        outputs_student = model(**student_inputs)\n",
    "        logits_student = outputs_student.logits\n",
    "\n",
    "        # CE loss (hard labels)\n",
    "        ce_loss = F.cross_entropy(logits_student, labels)\n",
    "\n",
    "        # Teacher logits\n",
    "        with torch.no_grad():\n",
    "            teacher_inputs = {k: v.to(DEVICE) for k, v in student_inputs.items()}\n",
    "            teacher_outputs = self.teacher(**teacher_inputs)\n",
    "            logits_teacher = teacher_outputs.logits\n",
    "\n",
    "        # Soft targets\n",
    "        T = self.temperature\n",
    "        student_log_probs = F.log_softmax(logits_student/T, dim=-1)\n",
    "        teacher_probs = F.softmax(logits_teacher/T, dim=-1)\n",
    "        kl = self.kl_loss(student_log_probs, teacher_probs)*(T*T)\n",
    "\n",
    "        loss = self.alpha * ce_loss + (1.0 - self.alpha) * kl\n",
    "\n",
    "        return (loss, outputs_student) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f5f154",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(-1)\n",
    "    return {\"accuracy\": metric.compute(predictions=preds, references=labels)[\"accuracy\"]}\n",
    "\n",
    "trainer = DistillationTrainer(\n",
    "    teacher_model=teacher,\n",
    "    temperature=temperature,\n",
    "    alpha=alpha,\n",
    "    model=student,\n",
    "    args=training_args,\n",
    "    train_dataset=train_datasets,\n",
    "    eval_dataset=val_datasets,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3451723e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0820e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Save the best model\n",
    "trainer.save_model(\"./drive/MyDrive/bert_sst2_student/best_model\")\n",
    "tokenizer.save_pretrained(\"./drive/MyDrive/bert_sst2_student/tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326ec1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 128\n",
    "import time\n",
    "# parameter counts\n",
    "def params_size_mb(model):\n",
    "    params = sum(p.numel() for p in model.parameters())\n",
    "    size_mb = params * 4 / (1024**2)\n",
    "    return params, size_mb\n",
    "\n",
    "s_params, s_size = params_size_mb(student)\n",
    "t_params, t_size = params_size_mb(teacher)\n",
    "print(f\"Student params: {s_params} ≈ {s_size:.1f} MB\")\n",
    "print(f\"Teacher params: {t_params} ≈ {t_size:.1f} MB\")\n",
    "\n",
    "# folder size of saved student\n",
    "def folder_size_mb(path):\n",
    "    total = 0\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for f in files:\n",
    "            total += os.path.getsize(os.path.join(root, f))\n",
    "    return total / (1024**2)\n",
    "\n",
    "print(\"Saved student folder size (MB):\", folder_size_mb(os.path.join(OUTPUT_DIR, \"best_student\")))\n",
    "\n",
    "# Latency measurement (batch=1)\n",
    "device = DEVICE\n",
    "student.to(device)\n",
    "student.eval()\n",
    "sample = tokenizer(\"This is a sample sentence to measure latency.\", return_tensors=\"pt\", max_length=max_len, truncation=True, padding=\"max_length\")\n",
    "input_ids = sample['input_ids'].to(device)\n",
    "attention_mask = sample['attention_mask'].to(device)\n",
    "\n",
    "# warmup\n",
    "with torch.no_grad():\n",
    "    for _ in range(10):\n",
    "        _ = student(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "N = 200\n",
    "torch.cuda.synchronize() if device==\"cuda\" else None\n",
    "t0 = time.time()\n",
    "with torch.no_grad():\n",
    "    for _ in range(N):\n",
    "        _ = student(input_ids=input_ids, attention_mask=attention_mask)\n",
    "if device==\"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "t1 = time.time()\n",
    "latency_ms = (t1 - t0) / N * 1000\n",
    "print(f\"Student batch=1 latency (avg over {N} runs): {latency_ms:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592e27b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./drive/MyDrive/bert_sst2_student/best_model\"\n",
    "tokenizer_path = \"./drive/MyDrive/bert_sst2_baseline/tokenizer\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, local_files_only=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1699b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "sentence = \"bad\"\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, return_token_type_ids=False).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    pred = probs.argmax(-1).item()\n",
    "\n",
    "print(\"Sentence:\", sentence)\n",
    "print(\"Prediction:\", pred, \"(probabilities:\", probs.cpu().numpy(), \")\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
