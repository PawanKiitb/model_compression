{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac667c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "MODEL_PATH = \"./drive/MyDrive/model_compression/head_pruned_student/best_model\"   # pruned (non-quantized) saved model\n",
    "QUANT_OUT =  \"./drive/MyDrive/model_compression/head_pruned_student_quantized\"    # where we'll save quantized model\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MAX_LEN = 128\n",
    "N_RUNS = 200\n",
    "\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd83830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, BitsAndBytesConfig\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "\n",
    "task = \"sst2\"\n",
    "raw = load_dataset(\"glue\", task)\n",
    "def preprocess(examples):\n",
    "    return tokenizer(examples[\"sentence\"], truncation=True, padding=\"max_length\", max_length=MAX_LEN)\n",
    "\n",
    "val_ds = raw[\"validation\"].map(preprocess, batched=True)\n",
    "if \"sentence\" in val_ds.column_names:\n",
    "    val_ds = val_ds.remove_columns([\"sentence\", \"idx\"])\n",
    "else:\n",
    "    cols_to_remove = [c for c in [\"sentence\",\"idx\"] if c in val_ds.column_names]\n",
    "    if cols_to_remove:\n",
    "        val_ds = val_ds.remove_columns(cols_to_remove)\n",
    "\n",
    "if \"label\" in val_ds.column_names and \"labels\" not in val_ds.column_names:\n",
    "    val_ds = val_ds.rename_column(\"label\", \"labels\")\n",
    "\n",
    "val_ds.set_format(type=\"torch\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "val_loader = DataLoader(val_ds, batch_size=32, collate_fn=data_collator)\n",
    "metric = load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fb0523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# DEVICE\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load in 16-bit float\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.float16,   # load all weights as FP16\n",
    "    device_map=\"auto\" if device==\"cuda\" else None\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Check some parameter dtypes\n",
    "cnt = 0\n",
    "for n, p in model.named_parameters():\n",
    "    print(n, p.device, p.dtype)\n",
    "    cnt += 1\n",
    "    if cnt > 6:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874bfaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(QUANT_OUT, exist_ok=True)\n",
    "print(\"Saving quantized model to:\", QUANT_OUT)\n",
    "model.save_pretrained(QUANT_OUT)\n",
    "tokenizer.save_pretrained(QUANT_OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ecc9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def folder_size_mb(path):\n",
    "    total = 0\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for f in files:\n",
    "            total += os.path.getsize(os.path.join(root, f))\n",
    "    return total / (1024**2)\n",
    "\n",
    "def count_params(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    return total\n",
    "\n",
    "total_params = count_params(model)\n",
    "print(f\"Parameter count: {total_params:,} (equivalent)\")\n",
    "print(f\"Saved quantized model folder size: {folder_size_mb(QUANT_OUT):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c9a3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds, all_labels = [], []\n",
    "\n",
    "model_device = next(model.parameters()).device\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        batch_inputs = {k: v.to(model_device) for k, v in batch.items() if k in [\"input_ids\", \"attention_mask\"]}\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        outputs = model(**batch_inputs)\n",
    "        preds = outputs.logits.argmax(-1).cpu()\n",
    "        all_preds.extend(preds.numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "val_acc = metric.compute(predictions=all_preds, references=all_labels)[\"accuracy\"]\n",
    "print(f\"Validation Accuracy: {val_acc*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee4a851",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_MODEL_PATH = \"./drive/MyDrive/model_compression/\"\n",
    "\n",
    "sample = tokenizer(\"This is a sample sentence to measure latency.\", return_tensors=\"pt\",\n",
    "                   max_length=MAX_LEN, truncation=True, padding=\"max_length\")\n",
    "input_ids = sample[\"input_ids\"].to(model_device)\n",
    "attention_mask = sample[\"attention_mask\"].to(model_device)\n",
    "\n",
    "# warmup\n",
    "with torch.no_grad():\n",
    "    for _ in range(10):\n",
    "        _ = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "if model_device.type == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "t0 = time.time()\n",
    "with torch.no_grad():\n",
    "    for _ in range(N_RUNS):\n",
    "        _ = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "if model_device.type == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "t1 = time.time()\n",
    "\n",
    "latency_ms = (t1 - t0)/N_RUNS*1000\n",
    "print(f\"Inference latency (batch=1, avg over {N_RUNS} runs): {latency_ms:.2f} ms\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
